{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Importing the models\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keeping track of constants for consistency\n",
    "\"\"\"\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.1\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Importing the dataset into the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Image batch: (64, 256, 256, 3)\n",
    "    # Labels batch: (64, 10)\n",
    "\n",
    "    # train\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/concatenated_datasets\",\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        subset=\"training\",\n",
    "        seed=SEED,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    # dev\n",
    "    dev_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"data/concatenated_datasets\",\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        subset=\"validation\",\n",
    "        seed=SEED,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    normalization_layer = layers.Rescaling(1.0 / 255)\n",
    "    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    dev_ds = dev_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "    return train_ds, dev_ds\n",
    "\n",
    "train_ds, dev_ds = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    \"\"\"\n",
    "    Making the actual model\n",
    "    \"\"\"\n",
    "\n",
    "    # weights_path = \"/scratch/st-sielmann-1/agrobot/grape-ld/pretrained_weights/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "    pre_trained_model = InceptionV3(\n",
    "        input_shape=(256, 256, 3), include_top=False, weights='imagenet'\n",
    "    )\n",
    "\n",
    "    for layer in pre_trained_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(pre_trained_model.output)\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(pre_trained_model.input, x)\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(learning_rate=0.0001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading the model weights. \n",
    "Note: Change checkpoint path to your path, and also you should have a .ckpt.index file and a .ckpt.data file\n",
    "\"\"\"\n",
    "checkpoint_path = \"training/cp-0080.ckpt\"\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the confusion matrix for error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "This is for the confusion matrix on the cross validation set\n",
    "\"\"\"\n",
    "# model.predict(dev_ds)\n",
    "\n",
    "predicted_dev_classes = np.array([])\n",
    "true_dev_classes =  np.array([])\n",
    "\n",
    "for x, y in dev_ds:\n",
    "    predicted_dev_classes = np.concatenate([predicted_dev_classes, np.argmax(model.predict(x), axis=-1)])\n",
    "    true_dev_classes = np.concatenate([true_dev_classes, np.argmax(y.numpy(), axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(dev_ds, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(true_dev_classes, predicted_dev_classes)\n",
    "\n",
    "class_names = ['BlackRot', 'DownyMildew', 'ESCA', 'GrayMold', 'Healthy', 'LeafBlight', 'MosaicVirus', 'Pierce', 'PowderyMildew', 'SourRot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Displaying the confusion matrix\n",
    "\"\"\"\n",
    "plt.figure(figsize=(9,9))\n",
    "seaborn.heatmap(confusion_matrix, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r', yticklabels=class_names, xticklabels=class_names)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "all_sample_title = f'Model Accuracy Score: {score[1]:.2%}'\n",
    "plt.title(all_sample_title, size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on webscraped images\n",
    "\n",
    "1. Clean the images of None images\n",
    "2. Load the dataset\n",
    "3. Get the predictions\n",
    "4. Display the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for cleaning the webscraped images.\n",
    "IGNORE THIS. \n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import imghdr\n",
    "\n",
    "data_dir = \"webscrape/PowderyMildew\"\n",
    "image_extensions = [\".jpg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(data_dir).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in image_extensions:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            os.system(f\"rm {filepath}\")\n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            os.system(f\"rm {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for loading the webscraped images\n",
    "IGNORE THIS. \n",
    "\"\"\"\n",
    "eval_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"webscrape\",\n",
    "    seed=SEED,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\",\n",
    ")\n",
    "normalization_layer = layers.Rescaling(1.0 / 255)\n",
    "eval_ds = eval_ds.map(lambda x, y: (normalization_layer(x), y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IGNORE THIS. \n",
    "\"\"\"\n",
    "# model.predict(dev_ds)\n",
    "\n",
    "predicted_eval_classes = np.array([])\n",
    "true_eval_classes =  np.array([])\n",
    "\n",
    "for x, y in eval_ds:\n",
    "    predicted_eval_classes = np.concatenate([predicted_eval_classes, np.argmax(model.predict(x), axis=-1)])\n",
    "    true_eval_classes = np.concatenate([true_eval_classes, np.argmax(y.numpy(), axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finding the accuracy of the webscraped images\n",
    "IGNORE THIS. \n",
    "\"\"\"\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(true_eval_classes)):\n",
    "    if true_eval_classes[i] == predicted_eval_classes[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
